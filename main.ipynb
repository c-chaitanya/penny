{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c-chaitanya/penny/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mvzuPozxmFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import requests \n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup as bs"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRG0lDqpxmFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    import shutil\n",
        "    current_file_path = os.getcwd()\n",
        "    path = os.path.join(current_file_path, \"Astro\")\n",
        "    shutil.rmtree(path)\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hyd96D2txmGL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f6504e90-44d4-4bae-a337-e24c06551d7b"
      },
      "source": [
        "df = pd.read_excel(\"Dataset - Penny test.xlsx\", sheet_name=\"Astro\")\n",
        "try:\n",
        "    # Downloading images to a folder and updating location in csv\n",
        "    current_file_path = os.getcwd()\n",
        "    folder_path = os.path.join(current_file_path, \"Astro\")\n",
        "    os.mkdir(folder_path)\n",
        "\n",
        "    df['Offline_image_path'] = \"\"\n",
        "\n",
        "    for i in df['Image']:\n",
        "        filename = i.split('/')[-1]\n",
        "        path = os.path.join(folder_path, filename)\n",
        "        urllib.request.urlretrieve(i, path)\n",
        "        df.loc[df['Image']==i,'Offline_image_path'] = str(path)\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    pass"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 17] File exists: '/content/Astro'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xia5sw4-xmGn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "b678c48d-7148-4622-9263-638b0cc6b4cc"
      },
      "source": [
        "# Scraping technical details about product\n",
        "page = 0\n",
        "for url in df['Identifier']:\n",
        "    print(url)\n",
        "    page +=1\n",
        "    r = requests.get(url)\n",
        "    if r.status_code != 200:\n",
        "        print(\"page does not exist\")\n",
        "        df.loc[df['Identifier'] == url,'description'] = \"\"\n",
        "        df.loc[df['Identifier'] == url,'specification'] = \"\"\n",
        "        df.loc[df['Identifier'] == url,'part_breakdown'] = \"\"\n",
        "        df.loc[df['Identifier'] == url,'product_info_sheet'] = \"\"\n",
        "        df.loc[df['Identifier'] == url,'marketing_flyer'] = \"\"\n",
        "        df.loc[df['Identifier'] == url,'related_products'] = \"\"\n",
        "\n",
        "    else:\n",
        "        print(\"========================================\")\n",
        "        content = r.content\n",
        "        soup = bs(content, 'html.parser')\n",
        "\n",
        "        try:\n",
        "            item = soup.find_all('div', {'class': 'value'})[0].text\n",
        "            df.loc[df['Identifier'] == url,'item'] = item\n",
        "        except Exception as e:\n",
        "            print(\"exception in item: \", e)\n",
        "            df.loc[df['Identifier'] == url,'item'] = \"\"\n",
        "        \n",
        "        try:\n",
        "            description = soup.find_all('div',  id=\"product.info.description\")[0].text\n",
        "            df.loc[df['Identifier'] == url,'description'] = description\n",
        "        except Exception as e:\n",
        "            print(\"exception in description: \", e)\n",
        "            df.loc[df['Identifier'] == url,'description'] = \"\"\n",
        "\n",
        "        try:\n",
        "            specification = soup.find_all('div',  id=\"product.info.description.attribute\")[0].text\n",
        "            df.loc[df['Identifier'] == url,'specification'] = specification\n",
        "        except Exception as e:\n",
        "            print(\"exception in specification: \", e)\n",
        "            df.loc[df['Identifier'] == url,'specification'] = specification\n",
        "\n",
        "        try:\n",
        "            part_breakdown = soup.find_all('div',  id=\"product.info.breakdown\")\n",
        "            link = part_breakdown[0].find_all('a', href=True)\n",
        "            pdf_url = link[0].get('href')\n",
        "            filename = \"part_breakdown_\" + pdf_url.split('/')[-1]\n",
        "            path = os.path.join(folder_path, filename)\n",
        "            urllib.request.urlretrieve(pdf_url, path)\n",
        "            df.loc[df['Identifier'] == url,'part_breakdown'] = str(path)\n",
        "        except Exception as e:\n",
        "            print(\"exception in part_breakdown: \", e)\n",
        "            df.loc[df['Identifier'] == url,'part_breakdown'] = \"\"\n",
        "\n",
        "        try:\n",
        "            product_info_sheet = soup.find_all('div',  id=\"product.info.sheet\")\n",
        "            link = product_info_sheet[0].find_all('a', href=True)\n",
        "            pdf_url = link[0].get('href')\n",
        "            filename = \"product_info_sheet_\" + pdf_url.split('/')[-1]\n",
        "            path = os.path.join(folder_path, filename)\n",
        "            urllib.request.urlretrieve(pdf_url, path)\n",
        "            df.loc[df['Identifier'] == url,'product_info_sheet'] = str(path)\n",
        "        except Exception as e:\n",
        "            print(\"exception in product_info_sheet: \", e)\n",
        "            df.loc[df['Identifier'] == url,'product_info_sheet'] = \"\"\n",
        "\n",
        "        try:\n",
        "            marketing_flyer = soup.find_all('div',  id=\"product.info.flyer\")\n",
        "            link = marketing_flyer[0].find_all('a', href=True)\n",
        "            pdf_url = link[0].get('href')\n",
        "            filename = \"marketing_flyer_\" + pdf_url.split('/')[-1]\n",
        "            path = os.path.join(folder_path, filename)\n",
        "            urllib.request.urlretrieve(pdf_url, path)\n",
        "            df.loc[df['Identifier'] == url,'marketing_flyer'] = str(path)\n",
        "        except Exception as e:\n",
        "            print(\"exception in marketing_flyer: \", e)\n",
        "            df.loc[df['Identifier'] == url,'marketing_flyer'] = \"\"\n",
        "   \n",
        "        try:\n",
        "            related_txt = \"\"\n",
        "            related_products = soup.find_all('li',  'item product product-item')\n",
        "            for cell in related_products:\n",
        "                cell1 = cell.find_all('a', href=True)\n",
        "                for link in cell1:\n",
        "                    l = link['href']\n",
        "                    images = link.find_all('img', alt=True)\n",
        "                    for image in images:\n",
        "                        i = image['src']\n",
        "                        related_text = i.split('/')[-1]\n",
        "                        related_txt = related_txt + \" item \" + related_text.split('.')[0] + \"\\n link:\" + l + \"\\n image:\" + i + \"\\n\\n\"\n",
        "            df.loc[df['Identifier'] == url,'related_products'] = str(related_txt)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"exception in related_products: \", e)\n",
        "            df.loc[df['Identifier'] == url,'related_products'] = \"\"\n",
        "\n",
        "        try:\n",
        "            #item = soup.find_all('a', {'class'='amazon action primary tocart'})[0].text\n",
        "            buy_online = soup.find('a', {'class':'amazon action primary tocart'})['href']\n",
        "            df.loc[df['Identifier'] == url,'buy_online'] = buy_online\n",
        "        except Exception as e:\n",
        "            print(\"exception in item: \", e)\n",
        "            df.loc[df['Identifier'] == url,'buy_online'] = \"\"    \n",
        "\n",
        "print(\"done\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://www.astrotools.com/air-tools/drills-screwdriviers/3-8-reversible-air-drill-1-800rpm.html\n",
            "page does not exist\n",
            "http://www.astrotools.com/1-2-extra-heavy-duty-reversible-air-drill-500rpm.html\n",
            "========================================\n",
            "http://www.astrotools.com/industrial-1-4-air-die-grinder.html\n",
            "========================================\n",
            "exception in marketing_flyer:  list index out of range\n",
            "http://www.astrotools.com/1-2-air-ratchet-wrench-50ft-lb-torque.html\n",
            "========================================\n",
            "exception in part_breakdown:  HTTP Error 404: Not Found\n",
            "exception in product_info_sheet:  HTTP Error 404: Not Found\n",
            "exception in marketing_flyer:  list index out of range\n",
            "http://www.astrotools.com/1-2-super-duty-impact-wrench-twin-hammer.html\n",
            "========================================\n",
            "exception in part_breakdown:  HTTP Error 404: Not Found\n",
            "exception in marketing_flyer:  list index out of range\n",
            "http://www.astrotools.com/1-heavy-duty-air-impact-wrench-with-2-anvil.html\n",
            "========================================\n",
            "exception in marketing_flyer:  list index out of range\n",
            "http://www.astrotools.com/1-heavy-duty-air-impact-wrench-with-6-anvil.html\n",
            "========================================\n",
            "exception in part_breakdown:  HTTP Error 404: Not Found\n",
            "exception in marketing_flyer:  list index out of range\n",
            "http://www.astrotools.com/onyx-heavy-duty-long-barrel-air-hammer-with-4pc-chisels-250mm.html\n",
            "========================================\n",
            "exception in marketing_flyer:  list index out of range\n",
            "http://www.astrotools.com/needle-scaler-flux-hammer-combo-4-400-blows-per-minute.html\n",
            "========================================\n",
            "exception in marketing_flyer:  list index out of range\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIJjNqqkxmHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1 = df"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTvmFJzYxmHn",
        "colab_type": "text"
      },
      "source": [
        "# Scrapping Knipex part using selenium"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XGDf_tNxmHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_excel(\"Dataset - Penny test.xlsx\", sheet_name=\"Knipex\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo-Gg-bdxmIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Downloading images to a folder and updating location in csv\n",
        "try:\n",
        "    current_file_path = os.getcwd()\n",
        "    folder_path = os.path.join(current_file_path, \"Knipex\")\n",
        "    os.mkdir(folder_path)\n",
        "\n",
        "    df['Offline_image_path'] = \"\"\n",
        "\n",
        "\n",
        "    for i in df['Image']:\n",
        "        filename = i.split('/')[-1]\n",
        "        path = os.path.join(folder_path, filename)\n",
        "        urllib.request.urlretrieve(i, path)\n",
        "        df.loc[df['Image']==i,'Offline_image_path'] = str(path)\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pezIjibxmIq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "da90dbbd-7a5a-4c4c-942f-60dc23bfbd74"
      },
      "source": [
        "# Scraping technical details about product\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import datetime\n",
        "\n",
        "driver = webdriver.Firefox()\n",
        "# Opening webpage\n",
        "driver.get(\"https://www.knipex.com/nc/en/home/\")\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    keyword = row['Identifier']\n",
        "    if isinstance(keyword, datetime.datetime):\n",
        "        df.loc[df['Identifier']==keyword, 'product_info'] = \"\"\n",
        "        df.loc[df['Identifier']==keyword, 'Description'] = \"\"\n",
        "        df.loc[df['Identifier']==keyword, 'Special'] = \"\"\n",
        "        df.loc[df['Identifier']==keyword, 'applications'] = \"\"\n",
        "        df.loc[df['Identifier']==keyword, 'pdf'] = \"\"\n",
        "        df.loc[df['Identifier']==keyword, 'video'] = \"\"\n",
        "        continue\n",
        "\n",
        "    sub_category = row['Sub-Category']\n",
        "    print(\"================================================\")\n",
        "    print(keyword)\n",
        "    print(sub_category)\n",
        "    # Finding the search bar\n",
        "    print(\"Searchbar\")\n",
        "    searchbar = driver.find_element_by_id(\"s\")\n",
        "    # Cleaning already existing text in search bar\n",
        "    searchbar.clear()\n",
        "    # Entering keyword to search in searchbar\n",
        "    print(\"entering data\")\n",
        "    searchbar.send_keys(keyword)\n",
        "    searchbar.send_keys(Keys.RETURN)\n",
        "    # Explicit wait for searched element to be available\n",
        "    try:\n",
        "        element = WebDriverWait(driver, 10).until(\n",
        "            EC.presence_of_element_located((By.ID, \"searchout\"))\n",
        "        )\n",
        "        # Searching for the matching category\n",
        "        try:\n",
        "            link = driver.find_element_by_link_text(sub_category)\n",
        "        except:\n",
        "            link = driver.find_element_by_xpath('//*[@id=\"searchout\"]')\n",
        "        # Clicking on returned link\n",
        "        print(\"clicking\")\n",
        "        link.click()\n",
        "\n",
        "        try:\n",
        "            product = driver.find_element_by_class_name('pgfinish')\n",
        "            #print(product.text)\n",
        "            df.loc[df['Identifier']==keyword, 'product_info'] = product.text\n",
        "        except Exception as e:\n",
        "            print(\"product error: \", e)\n",
        "            df.loc[df['Identifier']==keyword, 'product_info'] = \"\"\n",
        "        print(\"========================\")\n",
        "\n",
        "        try:\n",
        "            Description = driver.find_element_by_id('fragment-1')\n",
        "            #print(Description.text)\n",
        "            df.loc[df['Identifier']==keyword, 'Description'] = Description.text\n",
        "        except Exception as e:\n",
        "            print(\"description error: \", e)\n",
        "            df.loc[df['Identifier']==keyword, 'Description'] = \"\"\n",
        "        print(\"========================\")\n",
        "\n",
        "        try:\n",
        "            html = driver.page_source\n",
        "            soup = bs(html)\n",
        "            Special = soup.find_all('div',  id=\"fragment-2\")[0].text\n",
        "            df.loc[df['Identifier']==keyword, 'Special'] = Special\n",
        "        except Exception as e:\n",
        "            print(\"special error: \", e)\n",
        "            df.loc[df['Identifier']==keyword, 'Special'] = \"\"\n",
        "        print(\"========================\")\n",
        "\n",
        "        try:\n",
        "            html = driver.page_source\n",
        "            soup = bs(html)\n",
        "            applications = soup.find_all('div',  id=\"fragment-3\")[0].text\n",
        "            applications = soup.find_all('div',  {'class':\"artdetail_anw_img\"})\n",
        "            app = \"\"\n",
        "            for i in applications:\n",
        "                app = app + \" \" + i.text + \"\\n\"\n",
        "            df.loc[df['Identifier']==keyword, 'applications'] = app\n",
        "        except Exception as e:\n",
        "            print(\"application error: \", e)\n",
        "            df.loc[df['Identifier']==keyword, 'applications'] = \"\"\n",
        "        print(\"========================\")\n",
        "\n",
        "        \n",
        "        try:\n",
        "            link = driver.find_element_by_link_text(keyword)\n",
        "            link.click()\n",
        "            \n",
        "            html = driver.page_source\n",
        "            soup = bs(html)\n",
        "            technical_details = soup.find_all('div',  id=\"fragment-1\")[0].text\n",
        "            df.loc[df['Identifier']==keyword, 'technical_details'] = technical_details\n",
        "        except Exception as e:\n",
        "            print(\"technical_details error: \", e)\n",
        "            df.loc[df['Identifier']==keyword, 'technical_details'] = \"\"\n",
        "        print(\"========================\")\n",
        "\n",
        "        \n",
        "        try:\n",
        "            html = driver.page_source\n",
        "            soup = bs(html)\n",
        "            spare = soup.find_all('div', {'class':'pgJoins'})[0].text\n",
        "            df.loc[df['Identifier']==keyword, 'spare-accessories'] = spare\n",
        "        except Exception as e:\n",
        "            print(\"technical_details error: \", e)\n",
        "            df.loc[df['Identifier']==keyword, 'spare-accessories'] = \"\"\n",
        "        print(\"========================\")\n",
        "        \n",
        "        try:\n",
        "            html = driver.page_source\n",
        "            soup = bs(html)\n",
        "            pdf = soup.find_all('div',  id=\"fragment-7\")\n",
        "            link = pdf[0].find_all('a', href=True)\n",
        "            path = \"\"\n",
        "            for i in link:\n",
        "                pdf_url = i.get('href')\n",
        "                filename = pdf_url.split('/')[-1]\n",
        "                pat = os.path.join(folder_path, filename)\n",
        "                urllib.request.urlretrieve(\"http://www.knipex.com\"+pdf_url, pat)\n",
        "                path = path + pat + \"\\n\"\n",
        "            df.loc[df['Identifier']==keyword, 'pdf'] = str(path)\n",
        "        except Exception as e:\n",
        "            print(\"pdf error: \", e)\n",
        "            df.loc[df['Identifier']==keyword, 'pdf'] = \"\"\n",
        "        print(\"========================\")\n",
        "        \n",
        "        try:\n",
        "            html = driver.page_source\n",
        "            soup = bs(html)\n",
        "            #video = soup.find_all('div',  id=\"fragment-6\")\n",
        "            video = soup.find_all('iframe')\n",
        "            link = soup.find_all('iframe')[0]['src']\n",
        "            df.loc[df['Identifier']==keyword, 'video'] = str(link)\n",
        "        except Exception as e:\n",
        "            print(\"video error: \", e)\n",
        "            df.loc[df['Identifier']==keyword, 'video'] = \"\"\n",
        "            driver.back()\n",
        "        print(\"========================\")\n",
        "\n",
        "        # going back by 2 pages to the start page\n",
        "        driver.back()\n",
        "        driver.back()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        continue\n",
        "\n",
        "driver.quit()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-7d912041beea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Scraping technical details about product\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mby\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selenium'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXsSvpU9xmJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2 = df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A21XEdn4xmJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with pd.ExcelWriter('output.xlsx') as writer:  \n",
        "    df1.to_excel(writer, sheet_name='Astro')\n",
        "    df2.to_excel(writer, sheet_name='Knipex')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvD0eqbAxmKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}